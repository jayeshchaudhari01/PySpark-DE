{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0607c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"Access_key\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"Secret_key\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5361fe4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = spark.read.option(\"header\", True).csv(\"s3a://databricksbucket79/stream_input/\")\n",
    "importance_df = spark.read.option(\"header\", True).csv(\"s3a://databricksbucket79/CustomerImportance.csv\")\n",
    "importance_df = importance_df.withColumnRenamed(\"Source\", \"customer\")\n",
    "importance_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2635bd",
   "metadata": {},
   "source": [
    "PATD1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d87d7e",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "transactions_df = transactions_df.withColumn(\"amount\", col(\"amount\").cast(\"float\"))\n",
    "importance_df = importance_df.withColumn(\"weight\", col(\"weight\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb13811",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "merchant_txn_counts = transactions_df.groupBy(\"merchant\").count().filter(\"count > 50000\")\n",
    "txn_counts = merchant_txn_counts.groupBy(\"merchant\", \"customer\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b6037",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, percent_rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"merchant\").orderBy(col(\"count\").desc())\n",
    "\n",
    "# Add percent_rank to each customer\n",
    "ranked_customers = txn_counts.withColumn(\"percentile\", percent_rank().over(window_spec))\n",
    "#ranked_customers.show()\n",
    "top_1_percent_customers = ranked_customers.filter(col(\"percentile\") <= 0.01)\n",
    "#top_1_percent_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458507ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, percent_rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"merchant\").orderBy(col(\"count\").desc())\n",
    "\n",
    "# Add percent_rank to each customer\n",
    "ranked_customers = txn_counts.withColumn(\"percentile\", percent_rank().over(window_spec))\n",
    "#ranked_customers.show()\n",
    "top_1_percent_customers = ranked_customers.filter(col(\"percentile\") <= 0.01)\n",
    "#top_1_percent_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68509c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# [0.01] = 1st percentile\n",
    "# 0.0 = acceptable relative error\n",
    "percentile_value = importance_df.approxQuantile(\"weight\", [0.01], 0.0)[0]\n",
    "\n",
    "#print(f\"1st percentile of weight = {percentile_value}\")\n",
    "bottom_1_percent_weight_df = importance_df.filter(col(\"weight\") <= percentile_value)\n",
    "bottom_1_percent_weight_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd17c58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "final_upgrade = bottom_1_percent_weight_df.join(top_1_percent_customers, on=\"customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e27023",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "upgrade_detections = final_upgrade.select(\n",
    "    current_timestamp().alias(\"YStartTime\"),\n",
    "    current_timestamp().alias(\"detectionTime\"),\n",
    "    lit(\"PatId1\").alias(\"patternId\"),\n",
    "    lit(\"UPGRADE\").alias(\"ActionType\"),\n",
    "    col(\"customer\"),\n",
    "    col(\"merchant\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd506aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import floor, col\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Add index\n",
    "rdd_with_index = upgrade_detections.rdd.zipWithIndex()\n",
    "indexed_rdd = rdd_with_index.map(lambda row_index: Row(**row_index[0].asDict(), batch_id=int(row_index[1] // 50)))\n",
    "indexed_df = spark.createDataFrame(indexed_rdd)\n",
    "\n",
    "indexed_df.write.partitionBy(\"batch_id\").mode(\"overwrite\").option(\"header\", True) \\\n",
    "    .csv(\"s3a://databricksbucket79/output_detections/patid1_all_batches/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d7924",
   "metadata": {},
   "source": [
    "PATD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82648ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "transactions_df = transactions_df.withColumn(\"amount\", col(\"amount\").cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b977b77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg\n",
    "\n",
    "customer_merchant_stats = transactions_df.groupBy(\"merchant\", \"customer\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"txn_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "#customer_merchant_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00342905",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "patid2_df = customer_merchant_stats \\\n",
    "    .filter((col(\"txn_count\") >= 80) & (col(\"avg_amount\") < 23))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbc94f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "patid2_detections = patid2_df.select(\n",
    "    current_timestamp().alias(\"YStartTime\"),\n",
    "    current_timestamp().alias(\"detectionTime\"),\n",
    "    lit(\"PatId2\").alias(\"patternId\"),\n",
    "    lit(\"CHILD\").alias(\"ActionType\"),\n",
    "    col(\"customer\"),\n",
    "    col(\"merchant\")\n",
    ")\n",
    "#patid2_detections.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201cd80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import floor, col\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Add index\n",
    "rdd_with_index = patid2_detections.rdd.zipWithIndex()\n",
    "indexed_rdd = rdd_with_index.map(lambda row_index: Row(**row_index[0].asDict(), batch_id=int(row_index[1] // 50)))\n",
    "indexed_df = spark.createDataFrame(indexed_rdd)\n",
    "\n",
    "indexed_df.write.partitionBy(\"batch_id\").mode(\"overwrite\").option(\"header\", True) \\\n",
    "    .csv(\"s3a://databricksbucket79/output_detections/patid2_all_batches/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c7b9a",
   "metadata": {},
   "source": [
    "PATD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b0625",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transactions_df.select(\"gender\").distinct().show()\n",
    "valid_gender_df = transactions_df.filter(col(\"gender\").isin([\"'F'\", \"'M'\"]))\n",
    "#valid_gender_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887fe5a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "gender_stats = valid_gender_df.groupBy(\"merchant\").pivot(\"gender\").agg(countDistinct(\"customer\"))\n",
    "#gender_stats.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2e8c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gender_stats = gender_stats.fillna(0, subset=[\"'F'\", \"'M'\"])\n",
    "dei_needed_df = gender_stats.filter((col(\"'F'\") > 100) & (col(\"'F'\") < col(\"'M'\")))\n",
    "#dei_needed_df.show()\n",
    "new_dei = dei_needed_df.join(transactions_df, on=\"merchant\", how=\"inner\")\n",
    "new_dei.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132337ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "patid3_detections = new_dei.select(\n",
    "    current_timestamp().alias(\"YStartTime\"),\n",
    "    current_timestamp().alias(\"detectionTime\"),\n",
    "    lit(\"PatId3\").alias(\"patternId\"),\n",
    "    lit(\"DEI-NEEDED\").alias(\"ActionType\"),\n",
    "    col(\"customer\"),\n",
    "    col(\"merchant\")\n",
    ")\n",
    "#patid3_detections.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a06b3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import floor, col\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Add index\n",
    "rdd_with_index = patid3_detections.rdd.zipWithIndex()\n",
    "indexed_rdd = rdd_with_index.map(lambda row_index: Row(**row_index[0].asDict(), batch_id=int(row_index[1] // 50)))\n",
    "indexed_df = spark.createDataFrame(indexed_rdd)\n",
    "\n",
    "indexed_df.write.partitionBy(\"batch_id\").mode(\"overwrite\").option(\"header\", True) \\\n",
    "    .csv(\"s3a://databricksbucket79/output_detections/patid3_all_batches/\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
